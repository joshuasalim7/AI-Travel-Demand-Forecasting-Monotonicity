{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b929f0f-bb6f-4382-ba47-1378f8e3c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65007b2-3f42-4664-9bed-896ca06ccd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"./static_data_summary.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the 'totpop' column\n",
    "totpop = data['totpop'].values\n",
    "\n",
    "# Convert 'totpop' to a tensor and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "populations = torch.tensor(totpop, dtype=torch.float32).to(device)\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a417b86-b758-4f0c-80c9-332da66949c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target from the data\n",
    "features = data[['SerHourBusRoutes', 'SerHourRailRoutes', 'BusStopDen', 'RailStationDen', 'popden', 'pctmale', \n",
    "                 'pctbachelor', 'young2', 'pcthisp', 'carown', 'pctlowinc', 'pctmidinc', 'pcthighinc', \n",
    "                 'pctsinfam2', 'CrimeDen', 'RdNetwkDen', 'InterstDen']].values\n",
    "totpop = data['totpop'].values  #totpop is the target variable used for creating the monotonicity constraint\n",
    "# Monotonicty Constraint: as totpop increases, travel demand should increase \n",
    "# Predict 'travel_demand', use random values for now\n",
    "np.random.seed(0)\n",
    "travel_demand = np.random.rand(len(features)) * 1000  # Replace with actual travel demand data if available\n",
    "\n",
    "# Min-max normalization for features\n",
    "features_min = np.min(features, axis=0)\n",
    "features_max = np.max(features, axis=0)\n",
    "features_normalized = (features - features_min) / (features_max - features_min)\n",
    "\n",
    "# Min-max normalization for populations and travel_demand\n",
    "pop_min = np.min(totpop)\n",
    "pop_max = np.max(totpop)\n",
    "populations_normalized = (totpop - pop_min) / (pop_max - pop_min)\n",
    "\n",
    "travel_demand_min = np.min(travel_demand)\n",
    "travel_demand_max = np.max(travel_demand)\n",
    "travel_demand_normalized = (travel_demand - travel_demand_min) / (travel_demand_max - travel_demand_min)\n",
    "\n",
    "# Convert to tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = torch.tensor(features_normalized, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(travel_demand_normalized, dtype=torch.float32).to(device).view(-1, 1)\n",
    "populations = torch.tensor(populations_normalized, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e4f465-b069-4cc6-9371-0b554edfa1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022657b3-79b9-4353-96fe-20b728fad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes a penalty if the predictions do not uphold monotonic constraint \n",
    "def calculate_monotonicity_penalty(preds, populations, device):\n",
    "    penalty = torch.zeros(1, device=device)\n",
    "    for i in range(1, len(preds)):\n",
    "        if populations[i] > populations[i - 1] and preds[i] < preds[i - 1]:\n",
    "            penalty += torch.abs(preds[i] - preds[i - 1])\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbaf6ec3-9c5e-414c-b10b-8f020c33b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y_pred, label, criterion, lam, device, populations):\n",
    "    cost1 = criterion(y_pred, label).double()\n",
    "    monotonicity_penalty = calculate_monotonicity_penalty(y_pred, populations, device)\n",
    "    cost = (1 - lam) * cost1 + lam * monotonicity_penalty\n",
    "    return cost, cost1, monotonicity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cbfad9b-7f74-40ee-8cbd-0c7d5973875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.019421339035034, Cost1: 0.45822200179100037, Monotonicity Penalty: 5.580620765686035\n",
      "Epoch [2/100], Loss: 2.873983383178711, Cost1: 0.4535963535308838, Monotonicity Penalty: 5.294370651245117\n",
      "Epoch [3/100], Loss: 2.734501838684082, Cost1: 0.44924360513687134, Monotonicity Penalty: 5.0197601318359375\n",
      "Epoch [4/100], Loss: 2.599095582962036, Cost1: 0.44512513279914856, Monotonicity Penalty: 4.753066062927246\n",
      "Epoch [5/100], Loss: 2.4674437046051025, Cost1: 0.44104406237602234, Monotonicity Penalty: 4.4938435554504395\n",
      "Epoch [6/100], Loss: 2.343407154083252, Cost1: 0.4371221661567688, Monotonicity Penalty: 4.249691963195801\n",
      "Epoch [7/100], Loss: 2.222346544265747, Cost1: 0.43339407444000244, Monotonicity Penalty: 4.011299133300781\n",
      "Epoch [8/100], Loss: 2.1047897338867188, Cost1: 0.42983561754226685, Monotonicity Penalty: 3.7797436714172363\n",
      "Epoch [9/100], Loss: 1.993485927581787, Cost1: 0.4264475703239441, Monotonicity Penalty: 3.5605242252349854\n",
      "Epoch [10/100], Loss: 1.8926259279251099, Cost1: 0.42317625880241394, Monotonicity Penalty: 3.3620755672454834\n",
      "Epoch [11/100], Loss: 1.802931547164917, Cost1: 0.42003485560417175, Monotonicity Penalty: 3.18582820892334\n",
      "Epoch [12/100], Loss: 1.7179938554763794, Cost1: 0.41696783900260925, Monotonicity Penalty: 3.019019842147827\n",
      "Epoch [13/100], Loss: 1.643735408782959, Cost1: 0.41397106647491455, Monotonicity Penalty: 2.873499631881714\n",
      "Epoch [14/100], Loss: 1.5767815113067627, Cost1: 0.41094809770584106, Monotonicity Penalty: 2.742614984512329\n",
      "Epoch [15/100], Loss: 1.5147699117660522, Cost1: 0.4079887568950653, Monotonicity Penalty: 2.621551036834717\n",
      "Epoch [16/100], Loss: 1.4595495462417603, Cost1: 0.40516185760498047, Monotonicity Penalty: 2.51393723487854\n",
      "Epoch [17/100], Loss: 1.4081199169158936, Cost1: 0.402466744184494, Monotonicity Penalty: 2.4137730598449707\n",
      "Epoch [18/100], Loss: 1.364608883857727, Cost1: 0.3998644948005676, Monotonicity Penalty: 2.3293533325195312\n",
      "Epoch [19/100], Loss: 1.326145052909851, Cost1: 0.3974643349647522, Monotonicity Penalty: 2.2548258304595947\n",
      "Epoch [20/100], Loss: 1.289139747619629, Cost1: 0.39526429772377014, Monotonicity Penalty: 2.1830151081085205\n",
      "Epoch [21/100], Loss: 1.2526715993881226, Cost1: 0.3931849002838135, Monotonicity Penalty: 2.1121582984924316\n",
      "Epoch [22/100], Loss: 1.2157504558563232, Cost1: 0.3911536633968353, Monotonicity Penalty: 2.0403473377227783\n",
      "Epoch [23/100], Loss: 1.176745891571045, Cost1: 0.3891379237174988, Monotonicity Penalty: 1.9643537998199463\n",
      "Epoch [24/100], Loss: 1.1374483108520508, Cost1: 0.38713812828063965, Monotonicity Penalty: 1.8877583742141724\n",
      "Epoch [25/100], Loss: 1.0983779430389404, Cost1: 0.38515231013298035, Monotonicity Penalty: 1.8116036653518677\n",
      "Epoch [26/100], Loss: 1.0591938495635986, Cost1: 0.38325414061546326, Monotonicity Penalty: 1.7351335287094116\n",
      "Epoch [27/100], Loss: 1.0205225944519043, Cost1: 0.3815102279186249, Monotonicity Penalty: 1.6595350503921509\n",
      "Epoch [28/100], Loss: 0.9814717769622803, Cost1: 0.37983766198158264, Monotonicity Penalty: 1.5831059217453003\n",
      "Epoch [29/100], Loss: 0.9436495304107666, Cost1: 0.3782420754432678, Monotonicity Penalty: 1.5090569257736206\n",
      "Epoch [30/100], Loss: 0.9054635167121887, Cost1: 0.37682482600212097, Monotonicity Penalty: 1.434102177619934\n",
      "Epoch [31/100], Loss: 0.86833655834198, Cost1: 0.37550750374794006, Monotonicity Penalty: 1.3611656427383423\n",
      "Epoch [32/100], Loss: 0.8348758220672607, Cost1: 0.37423551082611084, Monotonicity Penalty: 1.2955161333084106\n",
      "Epoch [33/100], Loss: 0.8047659397125244, Cost1: 0.37298136949539185, Monotonicity Penalty: 1.2365505695343018\n",
      "Epoch [34/100], Loss: 0.7775290012359619, Cost1: 0.3716180920600891, Monotonicity Penalty: 1.1834399700164795\n",
      "Epoch [35/100], Loss: 0.7513872385025024, Cost1: 0.3701230585575104, Monotonicity Penalty: 1.132651448249817\n",
      "Epoch [36/100], Loss: 0.7278998494148254, Cost1: 0.3684440553188324, Monotonicity Penalty: 1.087355613708496\n",
      "Epoch [37/100], Loss: 0.7057962417602539, Cost1: 0.3666292428970337, Monotonicity Penalty: 1.0449632406234741\n",
      "Epoch [38/100], Loss: 0.6840747594833374, Cost1: 0.36471956968307495, Monotonicity Penalty: 1.0034300088882446\n",
      "Epoch [39/100], Loss: 0.6635762453079224, Cost1: 0.36270543932914734, Monotonicity Penalty: 0.964447021484375\n",
      "Epoch [40/100], Loss: 0.6433884501457214, Cost1: 0.3606833517551422, Monotonicity Penalty: 0.9260935187339783\n",
      "Epoch [41/100], Loss: 0.6232589483261108, Cost1: 0.3587132692337036, Monotonicity Penalty: 0.8878045678138733\n",
      "Epoch [42/100], Loss: 0.6039949059486389, Cost1: 0.3567764461040497, Monotonicity Penalty: 0.8512133359909058\n",
      "Epoch [43/100], Loss: 0.5866084694862366, Cost1: 0.3549249470233917, Monotonicity Penalty: 0.818291962146759\n",
      "Epoch [44/100], Loss: 0.5700674057006836, Cost1: 0.35327208042144775, Monotonicity Penalty: 0.7868627905845642\n",
      "Epoch [45/100], Loss: 0.5534178018569946, Cost1: 0.35183340311050415, Monotonicity Penalty: 0.7550021409988403\n",
      "Epoch [46/100], Loss: 0.5371708273887634, Cost1: 0.3505435883998871, Monotonicity Penalty: 0.7237980365753174\n",
      "Epoch [47/100], Loss: 0.522092878818512, Cost1: 0.3493276834487915, Monotonicity Penalty: 0.6948580741882324\n",
      "Epoch [48/100], Loss: 0.5099759697914124, Cost1: 0.3480755090713501, Monotonicity Penalty: 0.6718764305114746\n",
      "Epoch [49/100], Loss: 0.49814528226852417, Cost1: 0.34675630927085876, Monotonicity Penalty: 0.6495342254638672\n",
      "Epoch [50/100], Loss: 0.48640918731689453, Cost1: 0.3453271687030792, Monotonicity Penalty: 0.6274912357330322\n",
      "Epoch [51/100], Loss: 0.47506487369537354, Cost1: 0.34379979968070984, Monotonicity Penalty: 0.6063299775123596\n",
      "Epoch [52/100], Loss: 0.4655357301235199, Cost1: 0.34225261211395264, Monotonicity Penalty: 0.5888188481330872\n",
      "Epoch [53/100], Loss: 0.4565056264400482, Cost1: 0.340798556804657, Monotonicity Penalty: 0.5722126960754395\n",
      "Epoch [54/100], Loss: 0.44708433747291565, Cost1: 0.3394741415977478, Monotonicity Penalty: 0.5546945333480835\n",
      "Epoch [55/100], Loss: 0.43761467933654785, Cost1: 0.3383197784423828, Monotonicity Penalty: 0.5369095802307129\n",
      "Epoch [56/100], Loss: 0.428256094455719, Cost1: 0.3372780978679657, Monotonicity Penalty: 0.5192341208457947\n",
      "Epoch [57/100], Loss: 0.4194486737251282, Cost1: 0.3363471031188965, Monotonicity Penalty: 0.5025502443313599\n",
      "Epoch [58/100], Loss: 0.4106006920337677, Cost1: 0.3354978561401367, Monotonicity Penalty: 0.4857035279273987\n",
      "Epoch [59/100], Loss: 0.40195783972740173, Cost1: 0.3347015976905823, Monotonicity Penalty: 0.4692140817642212\n",
      "Epoch [60/100], Loss: 0.393595814704895, Cost1: 0.3338935673236847, Monotonicity Penalty: 0.45329806208610535\n",
      "Epoch [61/100], Loss: 0.38640907406806946, Cost1: 0.33305099606513977, Monotonicity Penalty: 0.43976715207099915\n",
      "Epoch [62/100], Loss: 0.37929168343544006, Cost1: 0.33221113681793213, Monotonicity Penalty: 0.426372230052948\n",
      "Epoch [63/100], Loss: 0.3720249533653259, Cost1: 0.3313769996166229, Monotonicity Penalty: 0.4126729369163513\n",
      "Epoch [64/100], Loss: 0.3651747703552246, Cost1: 0.33055341243743896, Monotonicity Penalty: 0.39979615807533264\n",
      "Epoch [65/100], Loss: 0.35954147577285767, Cost1: 0.3297550082206726, Monotonicity Penalty: 0.3893279433250427\n",
      "Epoch [66/100], Loss: 0.35289227962493896, Cost1: 0.3289535939693451, Monotonicity Penalty: 0.3768309950828552\n",
      "Epoch [67/100], Loss: 0.3459588885307312, Cost1: 0.32809802889823914, Monotonicity Penalty: 0.36381977796554565\n",
      "Epoch [68/100], Loss: 0.33993420004844666, Cost1: 0.3271397054195404, Monotonicity Penalty: 0.3527286946773529\n",
      "Epoch [69/100], Loss: 0.33408036828041077, Cost1: 0.3261435031890869, Monotonicity Penalty: 0.3420172333717346\n",
      "Epoch [70/100], Loss: 0.32838109135627747, Cost1: 0.3250202238559723, Monotonicity Penalty: 0.33174195885658264\n",
      "Epoch [71/100], Loss: 0.32272255420684814, Cost1: 0.3237818479537964, Monotonicity Penalty: 0.3216632306575775\n",
      "Epoch [72/100], Loss: 0.3171667754650116, Cost1: 0.32250460982322693, Monotonicity Penalty: 0.31182894110679626\n",
      "Epoch [73/100], Loss: 0.3117913007736206, Cost1: 0.3212984800338745, Monotonicity Penalty: 0.3022840917110443\n",
      "Epoch [74/100], Loss: 0.3064423203468323, Cost1: 0.32015976309776306, Monotonicity Penalty: 0.2927248477935791\n",
      "Epoch [75/100], Loss: 0.30149489641189575, Cost1: 0.3190980851650238, Monotonicity Penalty: 0.2838916778564453\n",
      "Epoch [76/100], Loss: 0.29654669761657715, Cost1: 0.3180745840072632, Monotonicity Penalty: 0.2750187814235687\n",
      "Epoch [77/100], Loss: 0.2918168902397156, Cost1: 0.3170715868473053, Monotonicity Penalty: 0.26656222343444824\n",
      "Epoch [78/100], Loss: 0.2876281142234802, Cost1: 0.31607839465141296, Monotonicity Penalty: 0.2591778635978699\n",
      "Epoch [79/100], Loss: 0.283263623714447, Cost1: 0.31499725580215454, Monotonicity Penalty: 0.2515299916267395\n",
      "Epoch [80/100], Loss: 0.2794662415981293, Cost1: 0.31381645798683167, Monotonicity Penalty: 0.24511602520942688\n",
      "Epoch [81/100], Loss: 0.27574628591537476, Cost1: 0.3126942813396454, Monotonicity Penalty: 0.23879826068878174\n",
      "Epoch [82/100], Loss: 0.2717900574207306, Cost1: 0.3116399049758911, Monotonicity Penalty: 0.23194020986557007\n",
      "Epoch [83/100], Loss: 0.2677355706691742, Cost1: 0.310604453086853, Monotonicity Penalty: 0.22486668825149536\n",
      "Epoch [84/100], Loss: 0.2638391852378845, Cost1: 0.30957484245300293, Monotonicity Penalty: 0.21810349822044373\n",
      "Epoch [85/100], Loss: 0.26000088453292847, Cost1: 0.3084736764431, Monotonicity Penalty: 0.21152812242507935\n",
      "Epoch [86/100], Loss: 0.2565463185310364, Cost1: 0.3072989583015442, Monotonicity Penalty: 0.20579367876052856\n",
      "Epoch [87/100], Loss: 0.2529803514480591, Cost1: 0.3060976266860962, Monotonicity Penalty: 0.19986310601234436\n",
      "Epoch [88/100], Loss: 0.2495918869972229, Cost1: 0.30488380789756775, Monotonicity Penalty: 0.19429998099803925\n",
      "Epoch [89/100], Loss: 0.2464001476764679, Cost1: 0.30366113781929016, Monotonicity Penalty: 0.18913917243480682\n",
      "Epoch [90/100], Loss: 0.24321356415748596, Cost1: 0.3022899031639099, Monotonicity Penalty: 0.1841372400522232\n",
      "Epoch [91/100], Loss: 0.24006468057632446, Cost1: 0.30083250999450684, Monotonicity Penalty: 0.17929686605930328\n",
      "Epoch [92/100], Loss: 0.23796264827251434, Cost1: 0.29945144057273865, Monotonicity Penalty: 0.17647385597229004\n",
      "Epoch [93/100], Loss: 0.2349683791399002, Cost1: 0.29814499616622925, Monotonicity Penalty: 0.17179176211357117\n",
      "Epoch [94/100], Loss: 0.23172521591186523, Cost1: 0.296974241733551, Monotonicity Penalty: 0.16647620499134064\n",
      "Epoch [95/100], Loss: 0.22898121178150177, Cost1: 0.2957626283168793, Monotonicity Penalty: 0.16219979524612427\n",
      "Epoch [96/100], Loss: 0.22671639919281006, Cost1: 0.29442232847213745, Monotonicity Penalty: 0.15901048481464386\n",
      "Epoch [97/100], Loss: 0.22443830966949463, Cost1: 0.29314813017845154, Monotonicity Penalty: 0.15572848916053772\n",
      "Epoch [98/100], Loss: 0.2221013456583023, Cost1: 0.2920222282409668, Monotonicity Penalty: 0.15218046307563782\n",
      "Epoch [99/100], Loss: 0.21991822123527527, Cost1: 0.2908135950565338, Monotonicity Penalty: 0.14902286231517792\n",
      "Epoch [100/100], Loss: 0.2176927924156189, Cost1: 0.289577454328537, Monotonicity Penalty: 0.1458081156015396\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Define model, criterion, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lam = 0.5  # Lambda for the monotonicity penalty\n",
    "# penalty_scaling_factor = 1e-9  # Scaling factor for the monotonicity penalty\n",
    "\n",
    "# Training loop\n",
    "# In each epoch, the model performs a forward pass, calculates the loss, computes gradients, and updates the parameters.\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss, cost1, monotonicity_penalty = calculate_loss(\n",
    "        y_pred, y_train, criterion, lam, device, populations\n",
    "    )\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for monitoring\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Cost1: {cost1.item()}, Monotonicity Penalty: {monotonicity_penalty.item()}')\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62593f9-8ef3-48d5-ab8e-7412a8395867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
