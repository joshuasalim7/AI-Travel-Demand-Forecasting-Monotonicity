{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e6c2d5-7361-46dc-9001-6e1baaea5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Downtown Column Distribution:\n",
      " is_downtown\n",
      "0    742\n",
      "1     29\n",
      "Name: count, dtype: int64\n",
      "Epoch [10/100], MSE: 0.0021\n",
      "Epoch [20/100], MSE: 0.0018\n",
      "Epoch [30/100], MSE: 0.0014\n",
      "Epoch [40/100], MSE: 0.0013\n",
      "Epoch [50/100], MSE: 0.0012\n",
      "Epoch [60/100], MSE: 0.0012\n",
      "Epoch [70/100], MSE: 0.0011\n",
      "Epoch [80/100], MSE: 0.0011\n",
      "Epoch [90/100], MSE: 0.0009\n",
      "Epoch [100/100], MSE: 0.0008\n",
      "\n",
      "Mean Squared Error (MSE): 2197058560.0000\n",
      "Mean Absolute Percentage Error (MAPE): 0.8588\n",
      "\n",
      "Results for Downtown Areas:\n",
      "                    Actual      Predicted\n",
      "index                                    \n",
      "17031081900   53560.996094  187097.265625\n",
      "17031080202  179386.000000  271374.343750\n",
      "17031081402  225875.000000  410526.968750\n",
      "17031320400  397826.000000  689930.562500\n",
      "17031080100  135940.000000  292310.250000\n",
      "17031839000  288007.000000  529001.812500\n",
      "\n",
      "Results for Some Other Areas:\n",
      "               Actual    Predicted\n",
      "index                             \n",
      "17031250800    9818.0      50.0000\n",
      "17031400500   17087.0      50.0000\n",
      "17031520600     258.0      50.0000\n",
      "17031252202   16582.0      50.0000\n",
      "17031841900  318842.0  380764.8125\n"
     ]
    }
   ],
   "source": [
    "# MLP Benchmark Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "merged_downtown_data = pd.read_csv('./merged_downtown_data.csv')\n",
    "ridesourcing_data = pd.read_csv(\"./Ridesourcing_CensusCount_ALL_0_Filled.csv\")\n",
    "\n",
    "# Renaming and setting index for merging\n",
    "merged_downtown_data = merged_downtown_data.rename(columns={\"TractID\": \"index\"})\n",
    "merged_downtown_data.set_index(\"index\", inplace=True)\n",
    "ridesourcing_data.set_index(\"index\", inplace=True)\n",
    "\n",
    "# Merging the dataframes\n",
    "merged_df = merged_downtown_data.join(ridesourcing_data, how='inner')\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "columns_to_drop = [\"Unnamed: 0\", \"X\", \"Y\"]\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Create the \"is_downtown\" column\n",
    "downtown_areas = [\n",
    "    '17031839000', '17031080202', '17031833000', '17031833100', '17031839100', \n",
    "    '17031081201', '17031080201', '17031081202', '17031842200', '17031838300', \n",
    "    '17031081401', '17031081403', '17031081300', '17031081100', '17031080300', \n",
    "    '17031080100', '17031080400', '17031081000', '17031320400', '17031320600', \n",
    "    '17031081402', '17031320100', '17031081900', '17031081500', '17031081800', \n",
    "    '17031081600', '17031081700', '17031280100', '17031281900'\n",
    "]\n",
    "\n",
    "downtown_areas = [int(tract_id) for tract_id in downtown_areas]\n",
    "\n",
    "merged_df['is_downtown'] = merged_df.index.isin(downtown_areas).astype(int)\n",
    "\n",
    "# Verify the 'is_downtown' column\n",
    "print(\"Is Downtown Column Distribution:\\n\", merged_df['is_downtown'].value_counts())\n",
    "\n",
    "# Aggregate the travel demand by summing across all time columns\n",
    "ridesourcing_data_aggregated = ridesourcing_data.sum(axis=1)\n",
    "\n",
    "# Splitting the data into features and aggregated target\n",
    "features = merged_df.drop(columns=ridesourcing_data.columns)\n",
    "target = ridesourcing_data_aggregated\n",
    "\n",
    "# Create MinMaxScaler instances for features and target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features and target\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Ensure deterministic split with stratification based on \"is_downtown\" column\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(features_scaled, target_scaled, features.index, test_size=0.2, stratify=merged_df['is_downtown'], random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_features = X_train.shape[1]\n",
    "seq_len = X_train.shape[1]\n",
    "pre_len = 1\n",
    "n_neurons = 64\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the MLP model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features, n_neurons, device, seq_len, pre_len):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.device = device\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons = n_neurons\n",
    "        self.seq_len = seq_len\n",
    "        self.pre_len = pre_len\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(seq_len, n_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(n_neurons, pre_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.01),\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        # x is a 2D tensor (batch_size, n_features)\n",
    "        x = x.reshape((-1, self.seq_len))\n",
    "        pred = self.linear_relu_stack(x).to(self.device)\n",
    "        pred = pred.reshape((-1, self.n_neurons))\n",
    "        return pred.float()\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        b_size = x.size(0)\n",
    "        x = x.reshape((-1, self.seq_len))\n",
    "        pred = self.linear_relu_stack(x).to(self.device)\n",
    "        pred = pred.reshape((b_size, -1))  # Reshape to (batch_size, output_features)\n",
    "        return pred.float()\n",
    "\n",
    "# Create the model instance\n",
    "model = Net(n_features, n_neurons, device, seq_len, pre_len).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range((X_train.size(0) - 1) // batch_size + 1):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, X_train.size(0))\n",
    "        \n",
    "        inputs = X_train[start_idx:end_idx, :].to(device)\n",
    "        targets = y_train[start_idx:end_idx].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test.to(device))\n",
    "            mse = criterion(y_pred, y_test.to(device))\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], MSE: {mse.item():.4f}')\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred.cpu().numpy().reshape(-1, 1))\n",
    "y_test_original = target_scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "# Calculate MSE and MAPE\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mape = mean_absolute_percentage_error(y_test_original, y_pred_original)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test_original.flatten(), 'Predicted': y_pred_original.flatten()}, index=index_test)\n",
    "\n",
    "# Print the results for all downtown areas and some other areas\n",
    "print(\"\\nResults for Downtown Areas:\")\n",
    "print(results_df[results_df.index.isin(downtown_areas)])\n",
    "\n",
    "print(\"\\nResults for Some Other Areas:\")\n",
    "other_areas = results_df[~results_df.index.isin(downtown_areas)].sample(5).index\n",
    "print(results_df[results_df.index.isin(other_areas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baf89ff-d773-40b7-ba65-23550867b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Downtown Column Distribution:\n",
      " is_downtown\n",
      "0    742\n",
      "1     29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mean Squared Error (MSE): 3923321098.2496\n",
      "Mean Absolute Percentage Error (MAPE): 16.3147\n",
      "\n",
      "Results for Downtown Areas:\n",
      "               Actual      Predicted\n",
      "index                               \n",
      "17031081900   53561.0  344741.405281\n",
      "17031080202  179386.0  473668.883472\n",
      "17031081402  225875.0  393177.677720\n",
      "17031320400  397826.0  674698.192450\n",
      "17031080100  135940.0  401206.169146\n",
      "17031839000  288007.0  616844.185044\n",
      "\n",
      "Results for Some Other Areas:\n",
      "              Actual      Predicted\n",
      "index                              \n",
      "17031200300   2130.0  -33245.086603\n",
      "17031030103  20203.0    6862.516583\n",
      "17031050200  82031.0   50829.303819\n",
      "17031841700   1249.0  -17376.685568\n",
      "17031831100  99091.0  107825.826000\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "merged_downtown_data = pd.read_csv('./merged_downtown_data.csv')\n",
    "ridesourcing_data = pd.read_csv(\"./Ridesourcing_CensusCount_ALL_0_Filled.csv\")\n",
    "\n",
    "# Renaming and setting index for merging\n",
    "merged_downtown_data = merged_downtown_data.rename(columns={\"TractID\": \"index\"})\n",
    "merged_downtown_data.set_index(\"index\", inplace=True)\n",
    "ridesourcing_data.set_index(\"index\", inplace=True)\n",
    "\n",
    "# Merging the dataframes\n",
    "merged_df = merged_downtown_data.join(ridesourcing_data, how='inner')\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "columns_to_drop = [\"Unnamed: 0\", \"X\", \"Y\"]\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Create the \"is_downtown\" column\n",
    "downtown_areas = [\n",
    "    '17031839000', '17031080202', '17031833000', '17031833100', '17031839100', \n",
    "    '17031081201', '17031080201', '17031081202', '17031842200', '17031838300', \n",
    "    '17031081401', '17031081403', '17031081300', '17031081100', '17031080300', \n",
    "    '17031080100', '17031080400', '17031081000', '17031320400', '17031320600', \n",
    "    '17031081402', '17031320100', '17031081900', '17031081500', '17031081800', \n",
    "    '17031081600', '17031081700', '17031280100', '17031281900'\n",
    "]\n",
    "\n",
    "downtown_areas = [int(tract_id) for tract_id in downtown_areas]\n",
    "\n",
    "merged_df['is_downtown'] = merged_df.index.isin(downtown_areas).astype(int)\n",
    "\n",
    "# Verify the 'is_downtown' column\n",
    "print(\"Is Downtown Column Distribution:\\n\", merged_df['is_downtown'].value_counts())\n",
    "\n",
    "# Aggregate the travel demand by summing across all time columns\n",
    "ridesourcing_data_aggregated = ridesourcing_data.sum(axis=1)\n",
    "\n",
    "# Splitting the data into features and aggregated target\n",
    "features = merged_df.drop(columns=ridesourcing_data.columns)\n",
    "target = ridesourcing_data_aggregated\n",
    "\n",
    "# Create MinMaxScaler instances for features and target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features and target\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Ensure deterministic split with stratification based on \"is_downtown\" column\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(features_scaled, target_scaled, features.index, test_size=0.2, stratify=merged_df['is_downtown'], random_state=42)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate MSE and MAPE\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mape = mean_absolute_percentage_error(y_test_original, y_pred_original)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test_original, 'Predicted': y_pred_original}, index=index_test)\n",
    "\n",
    "# Print the results for all downtown areas and some other areas\n",
    "print(\"\\nResults for Downtown Areas:\")\n",
    "print(results_df[results_df.index.isin(downtown_areas)])\n",
    "\n",
    "print(\"\\nResults for Some Other Areas:\")\n",
    "other_areas = results_df[~results_df.index.isin(downtown_areas)].sample(5).index\n",
    "print(results_df[results_df.index.isin(other_areas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b84440-1c62-4d74-8848-2662d045257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Downtown Column Distribution:\n",
      " is_downtown\n",
      "0    742\n",
      "1     29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mean Squared Error (MSE): 2711255094.9553\n",
      "Mean Absolute Percentage Error (MAPE): 2.7825\n",
      "\n",
      "Results for Downtown Areas:\n",
      "               Actual  Predicted\n",
      "index                           \n",
      "17031081900   53561.0  249805.66\n",
      "17031080202  179386.0  296901.04\n",
      "17031081402  225875.0  271395.47\n",
      "17031320400  397826.0  737212.16\n",
      "17031080100  135940.0  284684.28\n",
      "17031839000  288007.0  591158.98\n",
      "\n",
      "Results for Some Other Areas:\n",
      "              Actual  Predicted\n",
      "index                          \n",
      "17031150200  56225.0   32081.07\n",
      "17031842000  38639.0   46896.93\n",
      "17031710500  14672.0   23826.02\n",
      "17031400300  18902.0   20411.15\n",
      "17031190702   6251.0   14428.21\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "merged_downtown_data = pd.read_csv('./merged_downtown_data.csv')\n",
    "ridesourcing_data = pd.read_csv(\"./Ridesourcing_CensusCount_ALL_0_Filled.csv\")\n",
    "\n",
    "# Renaming and setting index for merging\n",
    "merged_downtown_data = merged_downtown_data.rename(columns={\"TractID\": \"index\"})\n",
    "merged_downtown_data.set_index(\"index\", inplace=True)\n",
    "ridesourcing_data.set_index(\"index\", inplace=True)\n",
    "\n",
    "# Merging the dataframes\n",
    "merged_df = merged_downtown_data.join(ridesourcing_data, how='inner')\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "columns_to_drop = [\"Unnamed: 0\", \"X\", \"Y\"]\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Create the \"is_downtown\" column\n",
    "downtown_areas = [\n",
    "    '17031839000', '17031080202', '17031833000', '17031833100', '17031839100', \n",
    "    '17031081201', '17031080201', '17031081202', '17031842200', '17031838300', \n",
    "    '17031081401', '17031081403', '17031081300', '17031081100', '17031080300', \n",
    "    '17031080100', '17031080400', '17031081000', '17031320400', '17031320600', \n",
    "    '17031081402', '17031320100', '17031081900', '17031081500', '17031081800', \n",
    "    '17031081600', '17031081700', '17031280100', '17031281900'\n",
    "]\n",
    "\n",
    "downtown_areas = [int(tract_id) for tract_id in downtown_areas]\n",
    "\n",
    "merged_df['is_downtown'] = merged_df.index.isin(downtown_areas).astype(int)\n",
    "\n",
    "# Verify the 'is_downtown' column\n",
    "print(\"Is Downtown Column Distribution:\\n\", merged_df['is_downtown'].value_counts())\n",
    "\n",
    "# Aggregate the travel demand by summing across all time columns\n",
    "ridesourcing_data_aggregated = ridesourcing_data.sum(axis=1)\n",
    "\n",
    "# Splitting the data into features and aggregated target\n",
    "features = merged_df.drop(columns=ridesourcing_data.columns)\n",
    "target = ridesourcing_data_aggregated\n",
    "\n",
    "# Create MinMaxScaler instances for features and target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features and target\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Ensure deterministic split with stratification based on \"is_downtown\" column\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(features_scaled, target_scaled, features.index, test_size=0.2, stratify=merged_df['is_downtown'], random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate MSE and MAPE\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mape = mean_absolute_percentage_error(y_test_original, y_pred_original)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test_original, 'Predicted': y_pred_original}, index=index_test)\n",
    "\n",
    "# Print the results for all downtown areas and some other areas\n",
    "print(\"\\nResults for Downtown Areas:\")\n",
    "print(results_df[results_df.index.isin(downtown_areas)])\n",
    "\n",
    "print(\"\\nResults for Some Other Areas:\")\n",
    "other_areas = results_df[~results_df.index.isin(downtown_areas)].sample(5).index\n",
    "print(results_df[results_df.index.isin(other_areas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592b801-cf68-4386-81ef-280aabedd6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
