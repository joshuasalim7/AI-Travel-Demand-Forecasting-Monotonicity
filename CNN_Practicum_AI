# CNN Model 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Layer, Dropout, Conv1D, GlobalAveragePooling1D, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import logging
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)
tf.random.set_seed(42)

logging.basicConfig(level=logging.INFO)

# Load and preprocess the dataset
file_path = './alldata_downtownTodowntown.csv'
data = pd.read_csv(file_path)
data = data.dropna()

X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])
y = data['total_number_trips'] / 101

is_downtown = X.iloc[:, -1] == 1
print(f"Total samples: {len(X)}")
print(f"Downtown samples: {sum(is_downtown)}")
print(f"Non-downtown samples: {sum(~is_downtown)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.iloc[:, -1], random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# Reshape the input data for CNN (add a channel dimension)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Stochastic Monotonicity Layer
class StochasticMonotonicityLayer(Layer):
    def __init__(self, lam, **kwargs):
        super(StochasticMonotonicityLayer, self).__init__(**kwargs)
        self.lam = lam

    def build(self, input_shape):
        self.w = self.add_weight(shape=(1,),
                                 initializer='ones',
                                 trainable=True,
                                 constraint=tf.keras.constraints.NonNeg(),
                                 name='monotonicity_weight')
        super(StochasticMonotonicityLayer, self).build(input_shape)

    def call(self, inputs):
        features, predictions = inputs
        is_downtown = features[:, -1:]

        # Generate random values for stochastic decision
        random_values = tf.random.uniform(tf.shape(is_downtown))

        # Apply stochastic monotonicity based on lambda
        stochastic_mask = tf.cast(random_values < self.lam, tf.float32)
        downtown_effect = self.w * is_downtown * stochastic_mask

        adjusted_predictions = predictions + downtown_effect
        return adjusted_predictions

# Create the CNN model with stochastic monotonicity
def create_stochastic_cnn_model(input_shape, lam):
    inputs = Input(shape=input_shape)
    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)
    x = Conv1D(32, kernel_size=3, activation='relu')(x)
    x = GlobalAveragePooling1D()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    predictions = Dense(1)(x)

    # Reshape the input for the StochasticMonotonicityLayer
    reshaped_inputs = Reshape((input_shape[0],))(inputs)

    outputs = StochasticMonotonicityLayer(lam)([reshaped_inputs, predictions])

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    return model

def calculate_monotonicity_satisfaction(X_test, predictions):
    is_downtown_test = X_test[:, -1]
    total_pairs = 0
    satisfied_pairs = 0

    for i in range(len(predictions)):
        for j in range(i+1, len(predictions)):
            if is_downtown_test[i] != is_downtown_test[j]:
                total_pairs += 1
                if (is_downtown_test[i] > is_downtown_test[j] and predictions[i] > predictions[j]) or \
                   (is_downtown_test[i] < is_downtown_test[j] and predictions[i] < predictions[j]):
                    satisfied_pairs += 1

    return satisfied_pairs / total_pairs if total_pairs > 0 else 1.0

# Train and evaluate the model
def train_and_evaluate_stochastic_cnn(lam, batch_size, num_runs=5):
    all_mse = []
    all_avg_downtown = []
    all_avg_non_downtown = []
    all_monotonicity_satisfaction = []

    for _ in range(num_runs):
        model = create_stochastic_cnn_model(X_train_reshaped.shape[1:], lam)
        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

        try:
            history = model.fit(X_train_reshaped, y_train_scaled, validation_data=(X_test_reshaped, y_test_scaled),
                                epochs=200, batch_size=batch_size, callbacks=[early_stopping], verbose=0)

            predictions_scaled = model.predict(X_test_reshaped)
            predictions = target_scaler.inverse_transform(predictions_scaled)

            mse = mean_squared_error(y_test, predictions)

            is_downtown_test = X_test.iloc[:, -1] == 1
            avg_downtown = np.mean(predictions[is_downtown_test])
            avg_non_downtown = np.mean(predictions[~is_downtown_test])

            monotonicity_satisfaction = calculate_monotonicity_satisfaction(X_test_scaled, predictions)

            all_mse.append(mse)
            all_avg_downtown.append(avg_downtown)
            all_avg_non_downtown.append(avg_non_downtown)
            all_monotonicity_satisfaction.append(monotonicity_satisfaction)

        except Exception as e:
            logging.error(f"Error during model training or evaluation: {str(e)}")

    if all_mse:
        return {
            'mse': np.mean(all_mse),
            'mse_std': np.std(all_mse),
            'avg_downtown': np.mean(all_avg_downtown),
            'avg_non_downtown': np.mean(all_avg_non_downtown),
            'monotonicity_satisfaction': np.mean(all_monotonicity_satisfaction),
            'monotonicity_satisfaction_std': np.std(all_monotonicity_satisfaction)
        }
    else:
        return None

# Test different values of lam
lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1]
batch_size = 64
results = []

for lam in lam_values:
    result = train_and_evaluate_stochastic_cnn(lam, batch_size)
    if result is not None:
        result['lambda'] = lam
        results.append(result)

        print(f"Lambda: {lam}")
        print(f"MSE: {result['mse']} ± {result['mse_std']}")
        print(f"Average prediction for downtown: {result['avg_downtown']}")
        print(f"Average prediction for non-downtown: {result['avg_non_downtown']}")
        print(f"Monotonicity satisfaction: {result['monotonicity_satisfaction']:.2%} ± {result['monotonicity_satisfaction_std']:.2%}")
        print("-" * 50)
    else:
        print(f"Skipping Lambda: {lam} due to error")

# Create a DataFrame from the results
results_df = pd.DataFrame(results)

# Plotting
plt.figure(figsize=(15, 10))

# MSE plot
plt.subplot(2, 2, 1)
plt.errorbar(results_df['lambda'], results_df['mse'], yerr=results_df['mse_std'], capsize=5, marker='o')
plt.title('MSE vs Lambda')
plt.xlabel('Lambda')
plt.ylabel('Mean Squared Error')

# Average predictions plot
plt.subplot(2, 2, 2)
plt.plot(results_df['lambda'], results_df['avg_downtown'], marker='o', label='Downtown')
plt.plot(results_df['lambda'], results_df['avg_non_downtown'], marker='o', label='Non-Downtown')
plt.title('Average Predictions vs Lambda')
plt.xlabel('Lambda')
plt.ylabel('Average Prediction')
plt.legend()

# Monotonicity satisfaction plot
plt.subplot(2, 2, 3)
plt.errorbar(results_df['lambda'], results_df['monotonicity_satisfaction'],
             yerr=results_df['monotonicity_satisfaction_std'], capsize=5, marker='o')
plt.title('Monotonicity Satisfaction vs Lambda')
plt.xlabel('Lambda')
plt.ylabel('Monotonicity Satisfaction')

# Heatmap of correlations
plt.subplot(2, 2, 4)
corr_df = results_df.drop(['mse_std', 'monotonicity_satisfaction_std'], axis=1)
sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')

plt.tight_layout()
plt.savefig('stochastic_monotonicity_cnn_analysis.png')
plt.close()

print("\nSummary of Results:")
print(results_df)
print("\nCorrelation Matrix:")
print(corr_df.corr())

# Calculate and print the actual average trip numbers
actual_avg_downtown = np.mean(y[is_downtown])
actual_avg_non_downtown = np.mean(y[~is_downtown])
print(f"\nActual average trip numbers:")
print(f"Downtown: {actual_avg_downtown}")
print(f"Non-downtown: {actual_avg_non_downtown}")

print("\nAnalysis saved as 'stochastic_monotonicity_cnn_analysis.png'")
