{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68433fba-fed3-4df0-80ed-fad77e55cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68aa4be3-0875-48ca-b5d1-2fe323faeee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"./static_data_summary.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the 'totpop' column\n",
    "totpop = data['totpop'].values\n",
    "\n",
    "# Extract the real travel demand data from ridesouring file \n",
    "ridesourcing_file_path = \"./Ridesourcing_CensusCount_ALL_0_Filled.csv\"\n",
    "ridesourcing_data = pd.read_csv(ridesourcing_file_path)\n",
    "\n",
    "# Aggregate travel demand data by summing all time intervals for each census tract\n",
    "travel_demand = ridesourcing_data.drop(columns=['index']).sum(axis=1).values\n",
    "\n",
    "# Convert 'totpop' to a tensor and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "populations = torch.tensor(totpop, dtype=torch.float32).to(device)\n",
    "travel_demand_tensor = torch.tensor(travel_demand, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5823b1c0-5b76-4d82-aafd-93b0036728e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the data\n",
    "features = data[['SerHourBusRoutes', 'SerHourRailRoutes', 'BusStopDen', 'RailStationDen', 'popden', 'pctmale', \n",
    "                 'pctbachelor', 'young2', 'pcthisp', 'carown', 'pctlowinc', 'pctmidinc', 'pcthighinc', \n",
    "                 'pctsinfam2', 'CrimeDen', 'RdNetwkDen', 'InterstDen']].values\n",
    "\n",
    "# Normalize the data\n",
    "# Min-max normalization for features, populations, and travel demand \n",
    "features_min = np.min(features, axis=0)\n",
    "features_max = np.max(features, axis=0)\n",
    "features_normalized = (features - features_min) / (features_max - features_min)\n",
    "\n",
    "pop_min = np.min(totpop)\n",
    "pop_max = np.max(totpop)\n",
    "populations_normalized = (totpop - pop_min) / (pop_max - pop_min)\n",
    "\n",
    "travel_demand_min = np.min(travel_demand)\n",
    "travel_demand_max = np.max(travel_demand)\n",
    "travel_demand_normalized = (travel_demand - travel_demand_min) / (travel_demand_max - travel_demand_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213ebcc3-7138-4065-a71c-ddf65ae851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train = torch.tensor(features_normalized, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(travel_demand_normalized, dtype=torch.float32).to(device).view(-1, 1)\n",
    "populations = torch.tensor(populations_normalized, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc678e9c-e36a-446c-aa55-4d8554cff9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model: Neural network with 1 hidden layer  \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547a7fd3-583e-4d0e-a38a-b37b47958b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes a monotonicity penalty if the predictions do not uphold monotonic constraint \n",
    "def calculate_monotonicity_penalty(preds, populations, device):\n",
    "    penalty = torch.zeros(1, device=device)\n",
    "    for i in range(1, len(preds)):\n",
    "        if populations[i] > populations[i - 1] and preds[i] < preds[i - 1]:\n",
    "            penalty += torch.abs(preds[i] - preds[i - 1])\n",
    "    return penalty\n",
    "\n",
    "# Loss function: combines the MSE loss with the monotonicity penalty \n",
    "def calculate_loss(y_pred, label, criterion, lam, device, populations):\n",
    "    cost1 = criterion(y_pred, label).double()\n",
    "    monotonicity_penalty = calculate_monotonicity_penalty(y_pred, populations, device)\n",
    "    cost = (1 - lam) * cost1 + lam * monotonicity_penalty # lam controls the trade-off between MSE loss and monotonicity penalty \n",
    "    return cost, cost1, monotonicity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edac38aa-9b0f-47a5-9229-b9cc2ac41d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.5521223545074463, Cost1: 0.011854110285639763, Monotonicity Penalty: 5.092390537261963\n",
      "Epoch [2/100], Loss: 2.403026580810547, Cost1: 0.011245746165513992, Monotonicity Penalty: 4.794807434082031\n",
      "Epoch [3/100], Loss: 2.2611329555511475, Cost1: 0.010689293965697289, Monotonicity Penalty: 4.5115766525268555\n",
      "Epoch [4/100], Loss: 2.125251054763794, Cost1: 0.010191548615694046, Monotonicity Penalty: 4.2403106689453125\n",
      "Epoch [5/100], Loss: 1.9948266744613647, Cost1: 0.009746846742928028, Monotonicity Penalty: 3.9799065589904785\n",
      "Epoch [6/100], Loss: 1.8702645301818848, Cost1: 0.00934935174882412, Monotonicity Penalty: 3.731179714202881\n",
      "Epoch [7/100], Loss: 1.753743290901184, Cost1: 0.008995863609015942, Monotonicity Penalty: 3.498490810394287\n",
      "Epoch [8/100], Loss: 1.6444907188415527, Cost1: 0.008681044913828373, Monotonicity Penalty: 3.2803003787994385\n",
      "Epoch [9/100], Loss: 1.5396002531051636, Cost1: 0.008396994322538376, Monotonicity Penalty: 3.07080340385437\n",
      "Epoch [10/100], Loss: 1.4395482540130615, Cost1: 0.00813660491257906, Monotonicity Penalty: 2.870959997177124\n",
      "Epoch [11/100], Loss: 1.345346212387085, Cost1: 0.007901879027485847, Monotonicity Penalty: 2.682790517807007\n",
      "Epoch [12/100], Loss: 1.2582204341888428, Cost1: 0.0076962802559137344, Monotonicity Penalty: 2.508744478225708\n",
      "Epoch [13/100], Loss: 1.1709935665130615, Cost1: 0.007514882367104292, Monotonicity Penalty: 2.334472179412842\n",
      "Epoch [14/100], Loss: 1.0879532098770142, Cost1: 0.00735129788517952, Monotonicity Penalty: 2.1685550212860107\n",
      "Epoch [15/100], Loss: 1.008845567703247, Cost1: 0.007200680207461119, Monotonicity Penalty: 2.0104904174804688\n",
      "Epoch [16/100], Loss: 0.9330856204032898, Cost1: 0.00706065446138382, Monotonicity Penalty: 1.8591105937957764\n",
      "Epoch [17/100], Loss: 0.8660944104194641, Cost1: 0.006928508635610342, Monotonicity Penalty: 1.7252602577209473\n",
      "Epoch [18/100], Loss: 0.8101065158843994, Cost1: 0.006802218966186047, Monotonicity Penalty: 1.6134108304977417\n",
      "Epoch [19/100], Loss: 0.7614841461181641, Cost1: 0.0066743893548846245, Monotonicity Penalty: 1.5162938833236694\n",
      "Epoch [20/100], Loss: 0.7170230746269226, Cost1: 0.006542261689901352, Monotonicity Penalty: 1.4275039434432983\n",
      "Epoch [21/100], Loss: 0.6844769716262817, Cost1: 0.006397716701030731, Monotonicity Penalty: 1.3625562191009521\n",
      "Epoch [22/100], Loss: 0.6615573167800903, Cost1: 0.006241919472813606, Monotonicity Penalty: 1.3168727159500122\n",
      "Epoch [23/100], Loss: 0.640721321105957, Cost1: 0.0060770269483327866, Monotonicity Penalty: 1.2753655910491943\n",
      "Epoch [24/100], Loss: 0.6213669776916504, Cost1: 0.005903768353164196, Monotonicity Penalty: 1.236830234527588\n",
      "Epoch [25/100], Loss: 0.6034027934074402, Cost1: 0.005729326047003269, Monotonicity Penalty: 1.2010762691497803\n",
      "Epoch [26/100], Loss: 0.5853629112243652, Cost1: 0.005562156904488802, Monotonicity Penalty: 1.1651636362075806\n",
      "Epoch [27/100], Loss: 0.5672881603240967, Cost1: 0.005402172915637493, Monotonicity Penalty: 1.1291741132736206\n",
      "Epoch [28/100], Loss: 0.5489760637283325, Cost1: 0.005251705646514893, Monotonicity Penalty: 1.092700481414795\n",
      "Epoch [29/100], Loss: 0.5307974815368652, Cost1: 0.0051178946159780025, Monotonicity Penalty: 1.0564770698547363\n",
      "Epoch [30/100], Loss: 0.5140479803085327, Cost1: 0.0050009628757834435, Monotonicity Penalty: 1.0230950117111206\n",
      "Epoch [31/100], Loss: 0.49750882387161255, Cost1: 0.004900176543742418, Monotonicity Penalty: 0.9901174902915955\n",
      "Epoch [32/100], Loss: 0.47991904616355896, Cost1: 0.0048129977658391, Monotonicity Penalty: 0.9550250768661499\n",
      "Epoch [33/100], Loss: 0.4622117877006531, Cost1: 0.004737836308777332, Monotonicity Penalty: 0.9196857213973999\n",
      "Epoch [34/100], Loss: 0.44519856572151184, Cost1: 0.004673932678997517, Monotonicity Penalty: 0.8857231736183167\n",
      "Epoch [35/100], Loss: 0.4308336675167084, Cost1: 0.004622544627636671, Monotonicity Penalty: 0.8570448160171509\n",
      "Epoch [36/100], Loss: 0.41798365116119385, Cost1: 0.004585135728120804, Monotonicity Penalty: 0.831382155418396\n",
      "Epoch [37/100], Loss: 0.40466636419296265, Cost1: 0.004557841923087835, Monotonicity Penalty: 0.8047748804092407\n",
      "Epoch [38/100], Loss: 0.39139893651008606, Cost1: 0.004539825022220612, Monotonicity Penalty: 0.7782580256462097\n",
      "Epoch [39/100], Loss: 0.37773576378822327, Cost1: 0.004533025901764631, Monotonicity Penalty: 0.7509384751319885\n",
      "Epoch [40/100], Loss: 0.36529994010925293, Cost1: 0.0045342640951275826, Monotonicity Penalty: 0.7260656356811523\n",
      "Epoch [41/100], Loss: 0.3539693355560303, Cost1: 0.004538262728601694, Monotonicity Penalty: 0.7034004330635071\n",
      "Epoch [42/100], Loss: 0.3428012728691101, Cost1: 0.004543989896774292, Monotonicity Penalty: 0.6810585260391235\n",
      "Epoch [43/100], Loss: 0.3324713706970215, Cost1: 0.00454696686938405, Monotonicity Penalty: 0.6603958010673523\n",
      "Epoch [44/100], Loss: 0.3220832049846649, Cost1: 0.004548961762338877, Monotonicity Penalty: 0.6396174430847168\n",
      "Epoch [45/100], Loss: 0.3115605413913727, Cost1: 0.004545228555798531, Monotonicity Penalty: 0.6185758709907532\n",
      "Epoch [46/100], Loss: 0.3014179766178131, Cost1: 0.004534593783318996, Monotonicity Penalty: 0.598301351070404\n",
      "Epoch [47/100], Loss: 0.29001906514167786, Cost1: 0.00451775174587965, Monotonicity Penalty: 0.575520396232605\n",
      "Epoch [48/100], Loss: 0.27963903546333313, Cost1: 0.004497693385928869, Monotonicity Penalty: 0.5547803640365601\n",
      "Epoch [49/100], Loss: 0.2698594629764557, Cost1: 0.004479596391320229, Monotonicity Penalty: 0.5352393388748169\n",
      "Epoch [50/100], Loss: 0.2605478763580322, Cost1: 0.004463158082216978, Monotonicity Penalty: 0.516632616519928\n",
      "Epoch [51/100], Loss: 0.25256285071372986, Cost1: 0.0044521004892885685, Monotonicity Penalty: 0.5006735920906067\n",
      "Epoch [52/100], Loss: 0.24498888850212097, Cost1: 0.004448059946298599, Monotonicity Penalty: 0.48552972078323364\n",
      "Epoch [53/100], Loss: 0.23752664029598236, Cost1: 0.004451064392924309, Monotonicity Penalty: 0.47060221433639526\n",
      "Epoch [54/100], Loss: 0.23090189695358276, Cost1: 0.004460266325622797, Monotonicity Penalty: 0.45734351873397827\n",
      "Epoch [55/100], Loss: 0.22459197044372559, Cost1: 0.0044727991335093975, Monotonicity Penalty: 0.4447111487388611\n",
      "Epoch [56/100], Loss: 0.2174338698387146, Cost1: 0.004486567806452513, Monotonicity Penalty: 0.430381178855896\n",
      "Epoch [57/100], Loss: 0.2113684117794037, Cost1: 0.004505415912717581, Monotonicity Penalty: 0.41823139786720276\n",
      "Epoch [58/100], Loss: 0.20611871778964996, Cost1: 0.004525467287749052, Monotonicity Penalty: 0.4077119827270508\n",
      "Epoch [59/100], Loss: 0.2018890380859375, Cost1: 0.004539600573480129, Monotonicity Penalty: 0.3992384672164917\n",
      "Epoch [60/100], Loss: 0.19701147079467773, Cost1: 0.004543863702565432, Monotonicity Penalty: 0.3894790709018707\n",
      "Epoch [61/100], Loss: 0.19205228984355927, Cost1: 0.004539025481790304, Monotonicity Penalty: 0.379565566778183\n",
      "Epoch [62/100], Loss: 0.18822190165519714, Cost1: 0.004532503429800272, Monotonicity Penalty: 0.37191128730773926\n",
      "Epoch [63/100], Loss: 0.18446438014507294, Cost1: 0.004526427946984768, Monotonicity Penalty: 0.36440232396125793\n",
      "Epoch [64/100], Loss: 0.18052129447460175, Cost1: 0.004521405790001154, Monotonicity Penalty: 0.3565211892127991\n",
      "Epoch [65/100], Loss: 0.17627422511577606, Cost1: 0.0045171212404966354, Monotonicity Penalty: 0.34803134202957153\n",
      "Epoch [66/100], Loss: 0.17236976325511932, Cost1: 0.00451566930860281, Monotonicity Penalty: 0.34022384881973267\n",
      "Epoch [67/100], Loss: 0.1681472212076187, Cost1: 0.004518991801887751, Monotonicity Penalty: 0.3317754566669464\n",
      "Epoch [68/100], Loss: 0.16453729569911957, Cost1: 0.004519618581980467, Monotonicity Penalty: 0.324554979801178\n",
      "Epoch [69/100], Loss: 0.1613689363002777, Cost1: 0.004516590852290392, Monotonicity Penalty: 0.3182212710380554\n",
      "Epoch [70/100], Loss: 0.15803363919258118, Cost1: 0.0045112487860023975, Monotonicity Penalty: 0.31155604124069214\n",
      "Epoch [71/100], Loss: 0.1545858234167099, Cost1: 0.004504432436078787, Monotonicity Penalty: 0.304667204618454\n",
      "Epoch [72/100], Loss: 0.15158939361572266, Cost1: 0.004500815644860268, Monotonicity Penalty: 0.2986779808998108\n",
      "Epoch [73/100], Loss: 0.14779646694660187, Cost1: 0.00450674444437027, Monotonicity Penalty: 0.29108619689941406\n",
      "Epoch [74/100], Loss: 0.1440810263156891, Cost1: 0.004520445596426725, Monotonicity Penalty: 0.28364160656929016\n",
      "Epoch [75/100], Loss: 0.14146748185157776, Cost1: 0.004532828461378813, Monotonicity Penalty: 0.2784021496772766\n",
      "Epoch [76/100], Loss: 0.1387328803539276, Cost1: 0.004539064131677151, Monotonicity Penalty: 0.2729266881942749\n",
      "Epoch [77/100], Loss: 0.1361684948205948, Cost1: 0.004539254121482372, Monotonicity Penalty: 0.2677977383136749\n",
      "Epoch [78/100], Loss: 0.13378989696502686, Cost1: 0.004534326959401369, Monotonicity Penalty: 0.26304545998573303\n",
      "Epoch [79/100], Loss: 0.13187645375728607, Cost1: 0.00453442707657814, Monotonicity Penalty: 0.2592184841632843\n",
      "Epoch [80/100], Loss: 0.12947465479373932, Cost1: 0.004539078567177057, Monotonicity Penalty: 0.2544102370738983\n",
      "Epoch [81/100], Loss: 0.1267019659280777, Cost1: 0.004546372685581446, Monotonicity Penalty: 0.2488575577735901\n",
      "Epoch [82/100], Loss: 0.12410509586334229, Cost1: 0.004552416503429413, Monotonicity Penalty: 0.24365776777267456\n",
      "Epoch [83/100], Loss: 0.12189692258834839, Cost1: 0.004557260777801275, Monotonicity Penalty: 0.23923657834529877\n",
      "Epoch [84/100], Loss: 0.1192002221941948, Cost1: 0.004553209058940411, Monotonicity Penalty: 0.2338472306728363\n",
      "Epoch [85/100], Loss: 0.11672286689281464, Cost1: 0.004552249796688557, Monotonicity Penalty: 0.2288934886455536\n",
      "Epoch [86/100], Loss: 0.11440407484769821, Cost1: 0.0045530167408287525, Monotonicity Penalty: 0.22425512969493866\n",
      "Epoch [87/100], Loss: 0.11223971098661423, Cost1: 0.004550465382635593, Monotonicity Penalty: 0.21992895007133484\n",
      "Epoch [88/100], Loss: 0.10994891077280045, Cost1: 0.0045541683211922646, Monotonicity Penalty: 0.2153436541557312\n",
      "Epoch [89/100], Loss: 0.1083197221159935, Cost1: 0.004555614199489355, Monotonicity Penalty: 0.2120838314294815\n",
      "Epoch [90/100], Loss: 0.1065346747636795, Cost1: 0.004552674014121294, Monotonicity Penalty: 0.2085166722536087\n",
      "Epoch [91/100], Loss: 0.10462605953216553, Cost1: 0.0045533562079072, Monotonicity Penalty: 0.20469875633716583\n",
      "Epoch [92/100], Loss: 0.10296281427145004, Cost1: 0.00455824239179492, Monotonicity Penalty: 0.20136739313602448\n",
      "Epoch [93/100], Loss: 0.10123224556446075, Cost1: 0.004558966960757971, Monotonicity Penalty: 0.1979055255651474\n",
      "Epoch [94/100], Loss: 0.0995219349861145, Cost1: 0.004557308740913868, Monotonicity Penalty: 0.1944865584373474\n",
      "Epoch [95/100], Loss: 0.09788648039102554, Cost1: 0.004556371830403805, Monotonicity Penalty: 0.1912165880203247\n",
      "Epoch [96/100], Loss: 0.09628746658563614, Cost1: 0.004557664971798658, Monotonicity Penalty: 0.18801726400852203\n",
      "Epoch [97/100], Loss: 0.09456627070903778, Cost1: 0.004562016110867262, Monotonicity Penalty: 0.1845705211162567\n",
      "Epoch [98/100], Loss: 0.09303873032331467, Cost1: 0.004567907191812992, Monotonicity Penalty: 0.18150955438613892\n",
      "Epoch [99/100], Loss: 0.09150893241167068, Cost1: 0.004572689533233643, Monotonicity Penalty: 0.17844517529010773\n",
      "Epoch [100/100], Loss: 0.08998130261898041, Cost1: 0.0045787133276462555, Monotonicity Penalty: 0.17538389563560486\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Define model, criterion, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lam = 0.5  # Lambda for the monotonicity penalty\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss, cost1, monotonicity_penalty = calculate_loss(\n",
    "        y_pred, y_train, criterion, lam, device, populations\n",
    "    )\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for monitoring\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Cost1: {cost1.item()}, Monotonicity Penalty: {monotonicity_penalty.item()}')\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cea2f-b25d-43cd-a87b-e2ccb7986784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
