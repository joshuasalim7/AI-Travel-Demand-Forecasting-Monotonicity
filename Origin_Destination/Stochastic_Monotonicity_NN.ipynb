{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Input layer --> 3 Hidden Layers (128 -> 64 -> 32 neurons) --> Dropout (0.3) --> Output Layer\n",
        "# Includes a custom MonotonicityLayer, STOCHASTIC MONOTONICITY, to enforce a soft constraint to the model\n",
        "# Adam optimizer, learning rate = 0.001\n",
        "# Loss function used is MSE, since it's a regression task\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Layer, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import logging\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load the dataset\n",
        "file_path = './alldata_downtownTodowntown.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Check for NaN values\n",
        "logging.info(f\"NaN values in the dataset:\\n{data.isna().sum()}\")\n",
        "\n",
        "# Handle NaN values\n",
        "data = data.dropna()  # This removes rows with any NaN values\n",
        "\n",
        "# Define our features and target\n",
        "X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])\n",
        "y = data['total_number_trips']\n",
        "\n",
        "# Check for NaN values again after preprocessing\n",
        "logging.info(f\"NaN values in X:\\n{X.isna().sum()}\")\n",
        "logging.info(f\"NaN values in y:\\n{y.isna().sum()}\")\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Normalize the target variable\n",
        "target_scaler = StandardScaler()\n",
        "y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Stochastic Monotonicity\n",
        "class MonotonicityLayer(Layer):\n",
        "    def __init__(self, lam, **kwargs):\n",
        "        super(MonotonicityLayer, self).__init__(**kwargs)\n",
        "        self.lam = lam\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(1,),\n",
        "                                 initializer='ones',\n",
        "                                 trainable=True,\n",
        "                                 name='monotonicity_weight')\n",
        "        super(MonotonicityLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features, predictions = inputs\n",
        "        is_downtown = features[:, -1:]  # Ensure is_downtown is a 2D tensor\n",
        "\n",
        "        # Soft monotonicity constraint\n",
        "        downtown_effect = self.lam * tf.math.sigmoid(self.w * is_downtown)\n",
        "        adjusted_predictions = predictions + downtown_effect\n",
        "\n",
        "        return adjusted_predictions\n",
        "\n",
        "# Create the model\n",
        "def create_model(input_shape, lam):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Dense(128, activation='relu')(inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1)(x)\n",
        "    outputs = MonotonicityLayer(lam)([inputs, predictions])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_and_evaluate(lam, batch_size):\n",
        "    model = create_model(X_train.shape[1], lam)\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "\n",
        "    try:\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                            epochs=100, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "        predictions_scaled = model.predict(X_test)\n",
        "        predictions = target_scaler.inverse_transform(predictions_scaled)\n",
        "\n",
        "        # Check the monotonicity constraint\n",
        "        is_downtown_test = X_test[:, -1] == 1\n",
        "        violations = 0\n",
        "        for i in range(len(predictions)):\n",
        "            for j in range(i+1, len(predictions)):\n",
        "                if is_downtown_test[i] > is_downtown_test[j] and predictions[i] < predictions[j]:\n",
        "                    violations += 1\n",
        "\n",
        "        mse = mean_squared_error(target_scaler.inverse_transform(y_test.reshape(-1, 1)), predictions)\n",
        "\n",
        "        return mse, violations, history\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model training or evaluation: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Test different values of lam\n",
        "lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "batch_size = 32\n",
        "results = []\n",
        "\n",
        "for lam in lam_values:\n",
        "    mse, violations, history = train_and_evaluate(lam, batch_size)\n",
        "    if mse is not None:\n",
        "        results.append((lam, mse, violations))\n",
        "        print(f\"Lambda: {lam}, MSE: {mse}, Violations: {violations}\")\n",
        "    else:\n",
        "        print(f\"Skipping Lambda: {lam} due to error\")\n",
        "\n",
        "print(\"\\nSummary of Results:\")\n",
        "for lam, mse, violations in results:\n",
        "    print(f\"Lambda: {lam}, MSE: {mse}, Violations: {violations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx6zzEmTKE8l",
        "outputId": "77441486-8c7e-4a08-96be-9adb5e8eae92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 0, MSE: 7808.737672199453, Violations: 0\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 0.2, MSE: 7283.332736935329, Violations: 0\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 0.4, MSE: 6791.940853676783, Violations: 0\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 0.6, MSE: 9699.987798592872, Violations: 0\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 0.8, MSE: 7754.885316485795, Violations: 0\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Lambda: 1, MSE: 7534.74428904993, Violations: 0\n",
            "\n",
            "Summary of Results:\n",
            "Lambda: 0, MSE: 7808.737672199453, Violations: 0\n",
            "Lambda: 0.2, MSE: 7283.332736935329, Violations: 0\n",
            "Lambda: 0.4, MSE: 6791.940853676783, Violations: 0\n",
            "Lambda: 0.6, MSE: 9699.987798592872, Violations: 0\n",
            "Lambda: 0.8, MSE: 7754.885316485795, Violations: 0\n",
            "Lambda: 1, MSE: 7534.74428904993, Violations: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxj6F7dct5Rr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}