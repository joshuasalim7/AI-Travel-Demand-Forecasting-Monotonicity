# CNN Model (Single-Feature Monotonicity)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Layer, Dropout, Conv1D, GlobalAveragePooling1D, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import logging

np.random.seed(42)
tf.random.set_seed(42)

logging.basicConfig(level=logging.INFO)

# Load and preprocess the dataset
file_path = './alldata_downtownTodowntown.csv'
data = pd.read_csv(file_path)
data = data.dropna()

X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])
y = data['total_number_trips'] / 101

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.iloc[:, -1], random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# Reshape the input data for CNN (add a channel dimension)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Stochastic Monotonicity Layer
class StochasticMonotonicityLayer(Layer):
    def __init__(self, lam, **kwargs):
        super(StochasticMonotonicityLayer, self).__init__(**kwargs)
        self.lam = lam

    def build(self, input_shape):
        self.monotonicity_weight = self.add_weight(shape=(1,),
                                                  initializer='ones',
                                                  trainable=True,
                                                  constraint=tf.keras.constraints.NonNeg(),
                                                  name='monotonicity_weight')
        super(StochasticMonotonicityLayer, self).build(input_shape)

    def call(self, inputs):
        features, predictions = inputs
        is_downtown = features[:, -1:]
        random_values = tf.random.uniform(tf.shape(is_downtown))
        stochastic_mask = tf.cast(random_values < self.lam, tf.float32)
        downtown_effect = self.monotonicity_weight * is_downtown * stochastic_mask
        adjusted_predictions = predictions + downtown_effect
        return adjusted_predictions

# Create the CNN model with stochastic monotonicity
def create_stochastic_cnn_model(input_shape, lam):
    inputs = Input(shape=input_shape)
    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)
    x = Conv1D(32, kernel_size=3, activation='relu')(x)
    x = GlobalAveragePooling1D()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.3)(x)
    predictions = Dense(1)(x)
    reshaped_inputs = Reshape((input_shape[0],))(inputs)
    outputs = StochasticMonotonicityLayer(lam)([reshaped_inputs, predictions])
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')
    return model

# Train and evaluate the model
def train_and_evaluate_stochastic_cnn(lam, batch_size, num_runs=5):
    all_mse = []

    for _ in range(num_runs):
        model = create_stochastic_cnn_model(X_train_reshaped.shape[1:], lam)
        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

        try:
            model.fit(X_train_reshaped, y_train_scaled, validation_data=(X_test_reshaped, y_test_scaled),
                      epochs=300, batch_size=batch_size, callbacks=[early_stopping], verbose=0)

            predictions_scaled = model.predict(X_test_reshaped)
            predictions = target_scaler.inverse_transform(predictions_scaled)

            mse = mean_squared_error(y_test, predictions)
            all_mse.append(mse)

        except Exception as e:
            logging.error(f"Error during model training or evaluation: {str(e)}")

    if all_mse:
        return np.mean(all_mse)
    else:
        return None

# Test different values of lam
lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1]
batch_size = 64
results = []

for lam in lam_values:
    mse = train_and_evaluate_stochastic_cnn(lam, batch_size)
    if mse is not None:
        results.append({'lambda': lam, 'mse': mse})
        print(f"Lambda: {lam}, MSE: {mse}")
    else:
        print(f"Skipping Lambda: {lam} due to error")

# Create a DataFrame from the results
results_df = pd.DataFrame(results)

print("\nSummary of Results:")
print(results_df)
