# Decision Tree Model
# Multi-feature Monotonicity Model 
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.base import BaseEstimator, RegressorMixin
import logging

np.random.seed(42)
logging.basicConfig(level=logging.INFO)

# The dataset
file_path = './alldata_downtownTodowntown.csv'
data = pd.read_csv(file_path)
data = data.dropna()
X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])
y = data['total_number_trips'] / 101

# Check the data of downtown vs non-downtown to make sure code is working
is_downtown = X.iloc[:, -1] == 1
print(f"Total samples: {len(X)}")
print(f"Downtown samples: {sum(is_downtown)}")
print(f"Non-downtown samples: {sum(~is_downtown)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.iloc[:, -1], random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# Define the monotonic features
monotonic_features = [
    'downtown_downtown',
    'EmpDen_Des', 'EmpDen_Ori',
    'Commuters_HW', 'Commuters_WH'
]
monotonic_indices = [X.columns.get_loc(col) for col in monotonic_features]

class MonotonicDecisionTree(BaseEstimator, RegressorMixin):
    def __init__(self, lam=0.5, max_depth=None, min_samples_split=2):
        self.lam = lam
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)

    def fit(self, X, y):
        # Add monotonic features to the target
        monotonic_features = X[:, monotonic_indices]
        monotonic_effect = self.lam * np.sum(monotonic_features, axis=1)
        modified_y = y - monotonic_effect
        
        self.tree.fit(X, modified_y)
        return self

    def predict(self, X):
        base_predictions = self.tree.predict(X)
        monotonic_features = X[:, monotonic_indices]
        monotonic_effect = self.lam * np.sum(monotonic_features, axis=1)
        return base_predictions + monotonic_effect

def train_and_evaluate_monotonic_tree(lam, max_depth, min_samples_split, num_runs=5):
    all_mse = []
    for _ in range(num_runs):
        model = MonotonicDecisionTree(lam=lam, max_depth=max_depth, min_samples_split=min_samples_split)
        try:
            model.fit(X_train_scaled, y_train_scaled)
            predictions_scaled = model.predict(X_test_scaled)
            predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
            mse = mean_squared_error(y_test, predictions)
            all_mse.append(mse)
        except Exception as e:
            logging.error(f"Error during model training or evaluation: {str(e)}")
    
    if all_mse:
        return np.mean(all_mse)
    else:
        return None

# Hyperparameters
lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1]
max_depth = 10
min_samples_split = 2

results = []
for lam in lam_values:
    mse = train_and_evaluate_monotonic_tree(lam, max_depth, min_samples_split)
    if mse is not None:
        results.append({'lambda': lam, 'mse': mse})
        print(f"Lambda: {lam}, MSE: {mse}")
    else:
        print(f"Skipping Lambda: {lam} due to error")

# Create a DF for the results
results_df = pd.DataFrame(results)
print("\nSummary of Results:")
print(results_df)

'''
Lambda: 0, MSE: 57.34890235040136
Lambda: 0.2, MSE: 56.15481701222001
Lambda: 0.4, MSE: 50.545999634700934
Lambda: 0.6, MSE: 57.64299736553553
Lambda: 0.8, MSE: 61.01627447197014
Lambda: 1, MSE: 53.00043212460177
'''
