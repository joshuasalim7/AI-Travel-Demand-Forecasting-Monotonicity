# ANN Single-Feature Model
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import logging

np.random.seed(42)
tf.random.set_seed(42)
logging.basicConfig(level=logging.INFO)

# Our dataset
file_path = './alldata_downtownTodowntown.csv'
data = pd.read_csv(file_path)
data = data.dropna()

X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])
y = data['total_number_trips'] / 101

# Check the data of downtown vs non-downtown
is_downtown = X.iloc[:, -1] == 1
print(f"Total samples: {len(X)}")
print(f"Downtown samples: {sum(is_downtown)}")
print(f"Non-downtown samples: {sum(~is_downtown)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.iloc[:, -1], random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# The monotonic feature
monotonic_feature = ['downtown_downtown']
monotonic_index = X.columns.get_loc(monotonic_feature[0])

class MonotonicANN:
    def __init__(self, input_dim, hidden_units=64, lam=0.5):
        self.input_dim = input_dim
        self.hidden_units = hidden_units
        self.lam = lam
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential([
            Dense(self.hidden_units, activation='relu', input_shape=(self.input_dim,)),
            Dense(1)
        ])
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        return model

    def fit(self, X, y, epochs=100, batch_size=32, verbose=0):
        monotonic_feature = X[:, monotonic_index]
        modified_y = y + self.lam * monotonic_feature
        self.model.fit(X, modified_y, epochs=epochs, batch_size=batch_size, verbose=verbose)

    def predict(self, X):
        base_predictions = self.model.predict(X).flatten()
        monotonic_feature = X[:, monotonic_index]
        return base_predictions - self.lam * monotonic_feature

def train_and_evaluate_monotonic_ann(lam, hidden_units=64, num_runs=5):
    all_mse = []
    for _ in range(num_runs):
        model = MonotonicANN(input_dim=X_train_scaled.shape[1], hidden_units=hidden_units, lam=lam)
        try:
            model.fit(X_train_scaled, y_train_scaled, epochs=200) # Increase the number of epochs
            predictions_scaled = model.predict(X_test_scaled)
            predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
            mse = mean_squared_error(y_test, predictions)
            all_mse.append(mse)
        except Exception as e:
            logging.error(f"Error during model training or evaluation: {str(e)}")

    if all_mse:
        return np.mean(all_mse)
    else:
        return None

# Hyperparameters
lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
hidden_units = 64

results = []
for lam in lam_values:
    mse = train_and_evaluate_monotonic_ann(lam, hidden_units)
    if mse is not None:
        results.append({'lambda': lam, 'mse': mse})
        print(f"Lambda: {lam}, MSE: {mse}")
    else:
        print(f"Skipping Lambda: {lam} due to error")

# Create a DF for the results
results_df = pd.DataFrame(results)
print("\nSummary of Results:")
print(results_df)
