# CNN Multi-Feature Model 
# Batch Normalization after each layer
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Layer, Dropout, Conv1D, GlobalAveragePooling1D, Reshape, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import logging

np.random.seed(42)
tf.random.set_seed(42)

logging.basicConfig(level=logging.INFO)

# Load the dataset
file_path = './alldata_downtownTodowntown.csv'
data = pd.read_csv(file_path)
data = data.dropna()

X = data.drop(columns=['total_number_trips', 'Unnamed: 0'])
y = data['total_number_trips'] / 101

# Check the data of downtown vs non-downtown
is_downtown = X.iloc[:, -1] == 1
print(f"Total samples: {len(X)}")
print(f"Downtown samples: {sum(is_downtown)}")
print(f"Non-downtown samples: {sum(~is_downtown)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.iloc[:, -1], random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# Reshape the input data for CNN by adding a channel dimension
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Define the monotonic features
monotonic_features = [
    'downtown_downtown',
    'EmpDen_Des', 'EmpDen_Ori',
    'Commuters_HW', 'Commuters_WH'
]

monotonic_indices = [X.columns.get_loc(col) for col in monotonic_features]

# Stochastic Monotonicity Layer
class StochasticMonotonicityLayer(Layer):
    def __init__(self, lam, num_monotonic_features, **kwargs):
        super(StochasticMonotonicityLayer, self).__init__(**kwargs)
        self.lam = lam
        self.num_monotonic_features = num_monotonic_features

    def build(self, input_shape):
        self.monotonicity_weights = self.add_weight(shape=(self.num_monotonic_features,),
                                                   initializer='ones',
                                                   trainable=True,
                                                   constraint=tf.keras.constraints.NonNeg(),
                                                   name='monotonicity_weights')
        super(StochasticMonotonicityLayer, self).build(input_shape)

    def call(self, inputs):
        features, predictions = inputs
        monotonic_features = tf.gather(features, monotonic_indices, axis=1)
        random_values = tf.random.uniform(tf.shape(monotonic_features))
        stochastic_mask = tf.cast(random_values < self.lam, tf.float32)
        monotonic_effects = tf.reduce_sum(self.monotonicity_weights * monotonic_features * stochastic_mask, axis=1, keepdims=True)
        adjusted_predictions = predictions + self.lam * monotonic_effects
        return adjusted_predictions

# Create the CNN model with stochastic monotonicity and batch normalization
def create_stochastic_cnn_model(input_shape, num_monotonic_features, lam):
    inputs = Input(shape=input_shape)
    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Conv1D(32, kernel_size=3, activation='relu')(x)
    x = BatchNormalization()(x)
    x = GlobalAveragePooling1D()(x)
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    predictions = Dense(1)(x)
    reshaped_inputs = Reshape((input_shape[0],))(inputs)
    outputs = StochasticMonotonicityLayer(lam, num_monotonic_features)([reshaped_inputs, predictions])
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')
    return model

# Train the CNN model
def train_and_evaluate_stochastic_cnn(lam, batch_size, num_runs=5):
    all_mse = []

    for _ in range(num_runs):
        model = create_stochastic_cnn_model(X_train_reshaped.shape[1:], len(monotonic_features), lam)
        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

        try:
            model.fit(X_train_reshaped, y_train_scaled, validation_data=(X_test_reshaped, y_test_scaled),
                      epochs=300, batch_size=batch_size, callbacks=[early_stopping], verbose=0)

            predictions_scaled = model.predict(X_test_reshaped)
            predictions = target_scaler.inverse_transform(predictions_scaled)

            mse = mean_squared_error(y_test, predictions)
            all_mse.append(mse)

        except Exception as e:
            logging.error(f"Error during model training or evaluation: {str(e)}")

    if all_mse:
        return np.mean(all_mse)
    else:
        return None

# Hyperparameters 
lam_values = [0, 0.2, 0.4, 0.6, 0.8, 1]
batch_size = 64
results = []

for lam in lam_values:
    mse = train_and_evaluate_stochastic_cnn(lam, batch_size)
    if mse is not None:
        results.append({'lambda': lam, 'mse': mse})
        print(f"Lambda: {lam}, MSE: {mse}")
    else:
        print(f"Skipping Lambda: {lam} due to error")

# Create a DataFrame from the results
results_df = pd.DataFrame(results)

print("\nSummary of Results:")
print(results_df)

'''
Convolutional layers: 2
Dense layers: 2
Batch normalization layers: 1 (only in the first convolutional block)
Dropout layers: 1
Custom layers: 1 (Stochastic Monotonicity Layer)
Total number of layers: 7
Hidden Layers: 3
Conv1D layer with 64 filters (Convolutional Block 1)
Conv1D layer with 32 filters (Convolutional Block 2)
Dense layer with 64 neurons (Dense Block)
Global Average Pooling Layer (can be considered a hidden layer, as it transforms the input)
The output of the Dense layer with 64 neurons is passed through the Stochastic Monotonicity Layer
'''
